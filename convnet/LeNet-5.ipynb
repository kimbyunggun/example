{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load mnist data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래프 초기화  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬 2와 파이썬 3 지원\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# 공통\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 일관된 출력을 위해 유사난수 초기화\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# 맷플롯립 설정\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# 한글출력\n",
    "plt.rcParams['font.family'] = 'NanumBarunGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 그림을 저장할 폴더\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"cnn\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mnist load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "reset_graph()\n",
    "#tf.keras.datasets.cifar10.load_data() \n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "#상태 변환 (28,28,1)\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28,28,1) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28,28,1) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image Shape: (28, 28, 1)\n",
      "\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
    "print()\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_valid)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST data that TensorFlow pre-loads comes as 28x28x1 images.\n",
    "\n",
    "However, the LeNet architecture only accepts 32x32xC images, where C is the number of color channels.\n",
    "\n",
    "In order to reformat the MNIST data into a shape that LeNet will accept, we pad the data with two rows of zeros on the top and bottom, and two columns of zeros on the left and right (28+2+2 = 32).\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Image Shape: (32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pad images with 0s np.pad\n",
    "X_train      = np.pad(X_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "X_valid      = np.pad(X_valid, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "X_test       = np.pad(X_test, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "    \n",
    "print(\"Updated Image Shape: {}\".format(X_train[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\matplotlib\\font_manager.py:1316: UserWarning: findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAF4AAABcCAYAAADnGgJlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYrlRZCYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20ddbad85f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "index = random.randint(0, len(X_train))\n",
    "#np.squeeze => Remove single-dimensional entries from the shape of an array.\n",
    "image = X_train[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "print(y_train[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shuffle process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Implement LeNet-5\n",
    "Implement the LeNet-5 neural network architecture.\n",
    "\n",
    "This is the only cell you need to edit.\n",
    "\n",
    "Input\n",
    "The LeNet architecture accepts a 32x32xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "Architecture\n",
    "Layer 1: Convolutional. The output shape should be 28x28x6.\n",
    "\n",
    "Activation. Your choice of activation function.\n",
    "\n",
    "Pooling. The output shape should be 14x14x6.\n",
    "\n",
    "Layer 2: Convolutional. The output shape should be 10x10x16.\n",
    "\n",
    "Activation. Your choice of activation function.\n",
    "\n",
    "Pooling. The output shape should be 5x5x16.\n",
    "\n",
    "Flatten. Flatten the output shape of the final pooling layer such that it's 1D instead of 3D. The easiest way to do is by using tf.contrib.layers.flatten, which is already imported for you.\n",
    "\n",
    "Layer 3: Fully Connected. This should have 120 outputs.\n",
    "\n",
    "Activation. Your choice of activation function.\n",
    "\n",
    "Layer 4: Fully Connected. This should have 84 outputs.\n",
    "\n",
    "Activation. Your choice of activation function.\n",
    "\n",
    "Layer 5: Fully Connected (Logits). This should have 10 outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x):    \n",
    "    # Hyperparameters\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    layer_depth = {\n",
    "        'layer_1' : 6,\n",
    "        'layer_2' : 16,\n",
    "        'layer_3' : 120,\n",
    "        'layer_f1' : 84\n",
    "    }\n",
    "\n",
    "    \n",
    "    # TODO: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    conv1_w = tf.Variable(tf.truncated_normal(shape = [5,5,1,6],mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    conv1 = tf.nn.conv2d(x,conv1_w, strides = [1,1,1,1], padding = 'VALID') + conv1_b \n",
    "    # TODO: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # TODO: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    pool_1 = tf.nn.max_pool(conv1,ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "    \n",
    "    # TODO: Layer 2: Convolutional. Output = 10x10x16.\n",
    "    conv2_w = tf.Variable(tf.truncated_normal(shape = [5,5,6,16], mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    conv2 = tf.nn.conv2d(pool_1, conv2_w, strides = [1,1,1,1], padding = 'VALID') + conv2_b\n",
    "    # TODO: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # TODO: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    pool_2 = tf.nn.max_pool(conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID') \n",
    "    \n",
    "    # TODO: Flatten. Input = 5x5x16. Output = 400.\n",
    "    fc1 = flatten(pool_2)\n",
    "    \n",
    "    # TODO: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fc1_w = tf.Variable(tf.truncated_normal(shape = (400,120), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1 = tf.matmul(fc1,fc1_w) + fc1_b\n",
    "    \n",
    "    # TODO: Activation.\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "    # TODO: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2_w = tf.Variable(tf.truncated_normal(shape = (120,84), mean = mu, stddev = sigma))\n",
    "    fc2_b = tf.Variable(tf.zeros(84))\n",
    "    fc2 = tf.matmul(fc1,fc2_w) + fc2_b\n",
    "    # TODO: Activation.\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    \n",
    "    # TODO: Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "    fc3_w = tf.Variable(tf.truncated_normal(shape = (84,10), mean = mu , stddev = sigma))\n",
    "    fc3_b = tf.Variable(tf.zeros(10))\n",
    "    logits = tf.matmul(fc2, fc3_w) + fc3_b\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    logits = LeNet(x)\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "0 배치 데이터 정확도: 0.9375 검증 세트 정확도: 0.9724\n",
      "1 배치 데이터 정확도: 0.9921875 검증 세트 정확도: 0.9812\n",
      "2 배치 데이터 정확도: 0.9765625 검증 세트 정확도: 0.985\n",
      "3 배치 데이터 정확도: 0.9921875 검증 세트 정확도: 0.9852\n",
      "4 배치 데이터 정확도: 1.0 검증 세트 정확도: 0.9844\n",
      "5 배치 데이터 정확도: 0.9921875 검증 세트 정확도: 0.9864\n",
      "6 배치 데이터 정확도: 0.9921875 검증 세트 정확도: 0.9878\n",
      "7 배치 데이터 정확도: 0.9921875 검증 세트 정확도: 0.9902\n",
      "8 배치 데이터 정확도: 0.9921875 검증 세트 정확도: 0.9874\n",
      "9 배치 데이터 정확도: 1.0 검증 세트 정확도: 0.9854\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Parent directory of lenet doesn't exist, can't save.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Failed to create a directory: ; No such file or directory\n\t [[Node: init_and_save_1/save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_init_and_save_1/save/Const_0_0, init_and_save_1/save/SaveV2/tensor_names, init_and_save_1/save/SaveV2/shape_and_slices, eval_1/Variable, eval_1/Variable_1, eval_1/Variable_2, eval_1/Variable_3, eval_1/Variable_4, eval_1/Variable_5, eval_1/Variable_6, eval_1/Variable_7, eval_1/Variable_8, eval_1/Variable_9, train_1/Variable, train_1/Variable/Adam, train_1/Variable/Adam_1, train_1/Variable_1, train_1/Variable_1/Adam, train_1/Variable_1/Adam_1, train_1/Variable_2, train_1/Variable_2/Adam, train_1/Variable_2/Adam_1, train_1/Variable_3, train_1/Variable_3/Adam, train_1/Variable_3/Adam_1, train_1/Variable_4, train_1/Variable_4/Adam, train_1/Variable_4/Adam_1, train_1/Variable_5, train_1/Variable_5/Adam, train_1/Variable_5/Adam_1, train_1/Variable_6, train_1/Variable_6/Adam, train_1/Variable_6/Adam_1, train_1/Variable_7, train_1/Variable_7/Adam, train_1/Variable_7/Adam_1, train_1/Variable_8, train_1/Variable_8/Adam, train_1/Variable_8/Adam_1, train_1/Variable_9, train_1/Variable_9/Adam, train_1/Variable_9/Adam_1, train_1/beta1_power, train_1/beta2_power, train_2/Variable, train_2/Variable/Adam, train_2/Variable/Adam_1, train_2/Variable_1, train_2/Variable_1/Adam, train_2/Variable_1/Adam_1, train_2/Variable_2, train_2/Variable_2/Adam, train_2/Variable_2/Adam_1, train_2/Variable_3, train_2/Variable_3/Adam, train_2/Variable_3/Adam_1, train_2/Variable_4, train_2/Variable_4/Adam, train_2/Variable_4/Adam_1, train_2/Variable_5, train_2/Variable_5/Adam, train_2/Variable_5/Adam_1, train_2/Variable_6, train_2/Variable_6/Adam, train_2/Variable_6/Adam_1, train_2/Variable_7, train_2/Variable_7/Adam, train_2/Variable_7/Adam_1, train_2/Variable_8, train_2/Variable_8/Adam, train_2/Variable_8/Adam_1, train_2/Variable_9, train_2/Variable_9/Adam, train_2/Variable_9/Adam_1, train_2/beta1_power, train_2/beta2_power)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[0;32m   1702\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1703\u001b[1;33m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[0;32m   1704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1334\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Failed to create a directory: ; No such file or directory\n\t [[Node: init_and_save_1/save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_init_and_save_1/save/Const_0_0, init_and_save_1/save/SaveV2/tensor_names, init_and_save_1/save/SaveV2/shape_and_slices, eval_1/Variable, eval_1/Variable_1, eval_1/Variable_2, eval_1/Variable_3, eval_1/Variable_4, eval_1/Variable_5, eval_1/Variable_6, eval_1/Variable_7, eval_1/Variable_8, eval_1/Variable_9, train_1/Variable, train_1/Variable/Adam, train_1/Variable/Adam_1, train_1/Variable_1, train_1/Variable_1/Adam, train_1/Variable_1/Adam_1, train_1/Variable_2, train_1/Variable_2/Adam, train_1/Variable_2/Adam_1, train_1/Variable_3, train_1/Variable_3/Adam, train_1/Variable_3/Adam_1, train_1/Variable_4, train_1/Variable_4/Adam, train_1/Variable_4/Adam_1, train_1/Variable_5, train_1/Variable_5/Adam, train_1/Variable_5/Adam_1, train_1/Variable_6, train_1/Variable_6/Adam, train_1/Variable_6/Adam_1, train_1/Variable_7, train_1/Variable_7/Adam, train_1/Variable_7/Adam_1, train_1/Variable_8, train_1/Variable_8/Adam, train_1/Variable_8/Adam_1, train_1/Variable_9, train_1/Variable_9/Adam, train_1/Variable_9/Adam_1, train_1/beta1_power, train_1/beta2_power, train_2/Variable, train_2/Variable/Adam, train_2/Variable/Adam_1, train_2/Variable_1, train_2/Variable_1/Adam, train_2/Variable_1/Adam_1, train_2/Variable_2, train_2/Variable_2/Adam, train_2/Variable_2/Adam_1, train_2/Variable_3, train_2/Variable_3/Adam, train_2/Variable_3/Adam_1, train_2/Variable_4, train_2/Variable_4/Adam, train_2/Variable_4/Adam_1, train_2/Variable_5, train_2/Variable_5/Adam, train_2/Variable_5/Adam_1, train_2/Variable_6, train_2/Variable_6/Adam, train_2/Variable_6/Adam_1, train_2/Variable_7, train_2/Variable_7/Adam, train_2/Variable_7/Adam_1, train_2/Variable_8, train_2/Variable_8/Adam, train_2/Variable_8/Adam_1, train_2/Variable_9, train_2/Variable_9/Adam, train_2/Variable_9/Adam_1, train_2/beta1_power, train_2/beta2_power)]]\n\nCaused by op 'init_and_save_1/save/SaveV2', defined at:\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-45-fe566f144196>\", line 3, in <module>\n    saver = tf.train.Saver()\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1338, in __init__\n    self.build()\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1347, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1384, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 832, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 350, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 266, in save_op\n    tensors)\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1800, in save_v2\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Failed to create a directory: ; No such file or directory\n\t [[Node: init_and_save_1/save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_init_and_save_1/save/Const_0_0, init_and_save_1/save/SaveV2/tensor_names, init_and_save_1/save/SaveV2/shape_and_slices, eval_1/Variable, eval_1/Variable_1, eval_1/Variable_2, eval_1/Variable_3, eval_1/Variable_4, eval_1/Variable_5, eval_1/Variable_6, eval_1/Variable_7, eval_1/Variable_8, eval_1/Variable_9, train_1/Variable, train_1/Variable/Adam, train_1/Variable/Adam_1, train_1/Variable_1, train_1/Variable_1/Adam, train_1/Variable_1/Adam_1, train_1/Variable_2, train_1/Variable_2/Adam, train_1/Variable_2/Adam_1, train_1/Variable_3, train_1/Variable_3/Adam, train_1/Variable_3/Adam_1, train_1/Variable_4, train_1/Variable_4/Adam, train_1/Variable_4/Adam_1, train_1/Variable_5, train_1/Variable_5/Adam, train_1/Variable_5/Adam_1, train_1/Variable_6, train_1/Variable_6/Adam, train_1/Variable_6/Adam_1, train_1/Variable_7, train_1/Variable_7/Adam, train_1/Variable_7/Adam_1, train_1/Variable_8, train_1/Variable_8/Adam, train_1/Variable_8/Adam_1, train_1/Variable_9, train_1/Variable_9/Adam, train_1/Variable_9/Adam_1, train_1/beta1_power, train_1/beta2_power, train_2/Variable, train_2/Variable/Adam, train_2/Variable/Adam_1, train_2/Variable_1, train_2/Variable_1/Adam, train_2/Variable_1/Adam_1, train_2/Variable_2, train_2/Variable_2/Adam, train_2/Variable_2/Adam_1, train_2/Variable_3, train_2/Variable_3/Adam, train_2/Variable_3/Adam_1, train_2/Variable_4, train_2/Variable_4/Adam, train_2/Variable_4/Adam_1, train_2/Variable_5, train_2/Variable_5/Adam, train_2/Variable_5/Adam_1, train_2/Variable_6, train_2/Variable_6/Adam, train_2/Variable_6/Adam_1, train_2/Variable_7, train_2/Variable_7/Adam, train_2/Variable_7/Adam_1, train_2/Variable_8, train_2/Variable_8/Adam, train_2/Variable_8/Adam_1, train_2/Variable_9, train_2/Variable_9/Adam, train_2/Variable_9/Adam_1, train_2/beta1_power, train_2/beta2_power)]]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-0c3e30a68890>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lenet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model saved\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[0;32m   1718\u001b[0m               \"Parent directory of {} doesn't exist, can't save.\".format(\n\u001b[0;32m   1719\u001b[0m                   save_path))\n\u001b[1;32m-> 1720\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1722\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrite_meta_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Parent directory of lenet doesn't exist, can't save."
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for epoch in range(n_epochs):\n",
    "        for x_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={x: x_batch, y: y_batch})\n",
    "                \n",
    "        acc_batch = accuracy.eval(feed_dict={x: x_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={x: X_valid, y: y_valid})\n",
    "        print(epoch, \"배치 데이터 정확도:\", acc_batch, \"검증 세트 정확도:\", acc_val)\n",
    "\n",
    "        \n",
    "    saver.save(sess, './lenet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
